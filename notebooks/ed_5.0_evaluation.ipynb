{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage you will test your model against previously unseen data and see how it compares to other models, which have been tested on the same set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler, PowerTransformer\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = os.path.abspath('../src/')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from ed_data_modeling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col_names=None, col_nums=None):\n",
    "        self.col_names=col_names\n",
    "        self.col_nums=col_nums\n",
    "        self.use = None\n",
    "        assert (self.col_names is not None) or (self.col_nums is not None), 'Must set either col_names or col_nums'\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.col_names is not None:\n",
    "            self.use = 'col_names'\n",
    "        elif self.col_nums is not None:\n",
    "            self.use = 'col_nums'\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.use == 'col_names':\n",
    "            _X = X[self.col_names].copy()\n",
    "        elif self.use == 'col_nums':\n",
    "            _X = X[:, self.col_nums]\n",
    "        return(_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'c3':'object',\n",
    "    'v1':'object',\n",
    "    'v2':'object',\n",
    "    'v3':'object',\n",
    "    'v4':'object',\n",
    "    'v5':'object',\n",
    "    'v6':'object',\n",
    "    'v7':'object',\n",
    "    'v8':'object',\n",
    "    'v9':'object',\n",
    "    'v10':'object',\n",
    "    'v11':'object',\n",
    "    'v12':'object',\n",
    "    'v13':'object',\n",
    "    'v14':'object',\n",
    "    'v15':'object',\n",
    "    'v17':'object',\n",
    "    'v18':'object',\n",
    "    'v19':'object',\n",
    "    'v21':'object',\n",
    "    'v22':'object',\n",
    "    'v23':'object',\n",
    "    'v24':'object',\n",
    "    'v29':'object',\n",
    "    'v31':'object',\n",
    "    'v34':'object',\n",
    "    'v35':'object',\n",
    "    'v36':'object',\n",
    "    'v39':'object',\n",
    "    'v41':'object',\n",
    "    'v42':'object',\n",
    "    'v43':'object',\n",
    "    'v44':'object',\n",
    "    'v45':'object',\n",
    "    'v46':'object',\n",
    "    'v47':'object',\n",
    "    'v48':'object',\n",
    "    'v49':'object',\n",
    "    'v50':'object',\n",
    "    'v51':'object',\n",
    "    'v52':'object',\n",
    "    'v53':'object',\n",
    "    'v54':'object',\n",
    "    'v55':'object',\n",
    "    'v57':'object',\n",
    "    'v58':'object',\n",
    "    'v59':'object',\n",
    "    'v60':'object',\n",
    "    'v61':'object',\n",
    "    'v62':'object',\n",
    "    'v63':'object',\n",
    "    'v64':'object',\n",
    "    'v65':'object',\n",
    "    'v66':'object',\n",
    "    'v67':'object',\n",
    "    'v68':'object',\n",
    "    'v69':'object',\n",
    "    'v70':'object',\n",
    "    'v71':'object',\n",
    "    'v72':'object',\n",
    "    'v73':'object',\n",
    "    'v74':'object',\n",
    "    'v75':'object',\n",
    "    'v76':'object',\n",
    "    'v77':'object',\n",
    "    'v78':'object',\n",
    "    'v79':'object',\n",
    "    'v80':'object',\n",
    "    'v81':'object',\n",
    "    'v82':'object',\n",
    "    'v83':'object',\n",
    "    'v84':'object',\n",
    "    'v85':'object',\n",
    "    'v86':'object',\n",
    "    'v87':'object',\n",
    "    'v88':'object',\n",
    "    'v89':'object',\n",
    "    'v90':'object',\n",
    "    'v91':'object',\n",
    "    'v92':'object',\n",
    "    'v93':'object',\n",
    "    'v94':'object',\n",
    "    'v98':'object',\n",
    "    'v100':'object',\n",
    "    'v104':'object',\n",
    "    'v107':'object',\n",
    "    'v108':'object',\n",
    "    'v109':'object',\n",
    "    'v110':'object',\n",
    "    'v111':'object',\n",
    "    'v112':'object',\n",
    "    'v113':'object',\n",
    "    'v114':'object',\n",
    "    'v115':'object',\n",
    "    'v116':'object',\n",
    "    'v117':'object',\n",
    "    'v118':'object',\n",
    "    'v119':'object',\n",
    "    'v120':'object',\n",
    "    'v121':'object',\n",
    "    'v122':'object',\n",
    "    'v123':'object',\n",
    "    'v124':'object',\n",
    "    'v125':'object',\n",
    "    'v138':'object',\n",
    "    'v139':'object',\n",
    "    'v140':'object',\n",
    "    'v141':'object',\n",
    "    'v142':'object',\n",
    "    'v146':'object',\n",
    "    'v147':'object',\n",
    "    'v148':'object',\n",
    "    'v149':'object',\n",
    "    'v153':'object',\n",
    "    'v154':'object',\n",
    "    'v155':'object',\n",
    "    'v156':'object',\n",
    "    'v157':'object',\n",
    "    'v158':'object',\n",
    "    'v169':'object',\n",
    "    'v170':'object',\n",
    "    'v172':'object',\n",
    "    'v173':'object',\n",
    "    'v174':'object',\n",
    "    'v175':'object',\n",
    "    'v176':'object',\n",
    "    'v181':'object',\n",
    "    'v183':'object',\n",
    "    'v184':'object',\n",
    "    'v185':'object',\n",
    "    'v186':'object',\n",
    "    'v188':'object',\n",
    "    'v189':'object',\n",
    "    'v190':'object',\n",
    "    'v191':'object',\n",
    "    'v192':'object',\n",
    "    'v193':'object',\n",
    "    'v194':'object',\n",
    "    'v195':'object',\n",
    "    'v196':'object',\n",
    "    'v197':'object',\n",
    "    'v198':'object',\n",
    "    'v199':'object',\n",
    "    'v200':'object',\n",
    "    'v220':'object',\n",
    "    'v223':'object',\n",
    "    'v235':'object',\n",
    "    'v236':'object',\n",
    "    'v237':'object',\n",
    "    'v238':'object',\n",
    "    'v239':'object',\n",
    "    'v241':'object',\n",
    "    'v242':'object',\n",
    "    'v244':'object',\n",
    "    'v246':'object',\n",
    "    'v247':'object',\n",
    "    'v249':'object',\n",
    "    'v250':'object',\n",
    "    'v251':'object',\n",
    "    'v252':'object',\n",
    "    'v257':'object',\n",
    "    'v260':'object',\n",
    "    'v262':'object',\n",
    "    'v281':'object',\n",
    "    'v282':'object',\n",
    "    'v284':'object',\n",
    "    'v286':'object',\n",
    "    'v287':'object',\n",
    "    'v288':'object',\n",
    "    'v289':'object',\n",
    "    'v297':'object',\n",
    "    'v300':'object',\n",
    "    'v301':'object',\n",
    "    'v302':'object',\n",
    "    'v303':'object',\n",
    "    'v304':'object',\n",
    "    'v305':'object',\n",
    "    'v325':'object',\n",
    "    'v326':'object',\n",
    "    'v327':'object',\n",
    "    'v328':'object',\n",
    "    'id_32':'object'\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the test data sets from the raw data folder (../data/raw/test_identity.csv and ../data/raw/test_transaction.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transactiondt</th>\n",
       "      <th>transactionamt</th>\n",
       "      <th>productcd</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>...</th>\n",
       "      <th>id_29</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transactionid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3663549</th>\n",
       "      <td>18403224</td>\n",
       "      <td>31.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663550</th>\n",
       "      <td>18403263</td>\n",
       "      <td>49.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>299.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663551</th>\n",
       "      <td>18403310</td>\n",
       "      <td>171.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>472.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663552</th>\n",
       "      <td>18403310</td>\n",
       "      <td>284.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>205.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663553</th>\n",
       "      <td>18403317</td>\n",
       "      <td>67.95</td>\n",
       "      <td>W</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>264.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 433 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               transactiondt  transactionamt productcd  card1  card2  card3  \\\n",
       "transactionid                                                                 \n",
       "3663549             18403224           31.95         W  10409  111.0  150.0   \n",
       "3663550             18403263           49.00         W   4272  111.0  150.0   \n",
       "3663551             18403310          171.00         W   4476  574.0  150.0   \n",
       "3663552             18403310          284.95         W  10989  360.0  150.0   \n",
       "3663553             18403317           67.95         W  18018  452.0  150.0   \n",
       "\n",
       "                    card4  card5  card6  addr1  ...  id_29  id_30  id_31  \\\n",
       "transactionid                                   ...                        \n",
       "3663549              visa  226.0  debit  170.0  ...    NaN    NaN    NaN   \n",
       "3663550              visa  226.0  debit  299.0  ...    NaN    NaN    NaN   \n",
       "3663551              visa  226.0  debit  472.0  ...    NaN    NaN    NaN   \n",
       "3663552              visa  166.0  debit  205.0  ...    NaN    NaN    NaN   \n",
       "3663553        mastercard  117.0  debit  264.0  ...    NaN    NaN    NaN   \n",
       "\n",
       "              id_32 id_33  id_34  id_35 id_36  id_37  id_38  \n",
       "transactionid                                                \n",
       "3663549         NaN   NaN    NaN    NaN   NaN    NaN    NaN  \n",
       "3663550         NaN   NaN    NaN    NaN   NaN    NaN    NaN  \n",
       "3663551         NaN   NaN    NaN    NaN   NaN    NaN    NaN  \n",
       "3663552         NaN   NaN    NaN    NaN   NaN    NaN    NaN  \n",
       "3663553         NaN   NaN    NaN    NaN   NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 433 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('../data/interim/df_test.csv', index_col='transactionid', dtype=dtype_dict)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df_test.select_dtypes(include='object').columns.to_list()\n",
    "numeric_columns = [c for c in df_test.columns if c not in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_ohe = [c for c in categorical_columns if df_test[c].nunique() <= 5]\n",
    "categorical_columns_te = [c for c in categorical_columns if df_test[c].nunique() >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = joblib.load('../models/best_model_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same pipeline (including any cleaning and preprocessing steps you performed on the training set), generate predictions for the samples in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eric.Denbin\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe.predict_proba(df_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'TransactionID' : df_test.index,\n",
    "    'isFraud' : predictions\n",
    "}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your predictions as a .csv file. Compare your predictions with ../data/raw/sample_submission.csv to ensure you've done this correctly (your file should look the same, but have different predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('ed_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have your model scored, you will need to submit your data on the kaggle competition. Though the competition is closed, it will still accept predictions and score models. So score your model, follow these steps:\n",
    "\n",
    "1. Go to the [Kaggle Competition Page](https://www.kaggle.com/c/ieee-fraud-detection)\n",
    "2. If you have a Kaggle account, sign in. If you do not have a Kaggle account, click the Register button on the top right of the page and make an account.\n",
    "3. Click the \"Late Submission\" button beneath the competition header on the right side.\n",
    "4. Upload the .csv file that you generated in the above steps.\n",
    "5. Click \"Make Submission\" and wait for your results to be evaluated.\n",
    "\n",
    "Once these steps are complete you should have access to two things:\n",
    "1. Your model's score.\n",
    "2. Your model's place on the leaderboard.\n",
    "\n",
    "Enter these two numbers below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private Score: 0.891836\n",
    "Rank: 4666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can repeat this process with multiple models to see how they compare to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous evaluation steps dealt with factors such as the accuracy and generality of the model. This step assesses the degree to which the model meets the business objectives and seeks to determine if there is some business reason why this model is deficient. \n",
    "\n",
    "Moreover, evaluation also assesses other data mining results generated. Data mining results involve models that are necessarily related to the original business objectives and all other findings that are not necessarily related to the original business objectives, but might also unveil additional challenges, information, or hints for future directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Assessment of data mining results with respect to business success criteria</b>\n",
    "- Summarize assessment results in terms of business success criteria (as established during the first phase), including a final statement regarding whether the project already meets the initial business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model results met the business criteria as established during the first phase in that the model sought to maximize area under the ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Approved models</b>\n",
    "- After assessing models with respect to business success criteria, the generated models that meet the selected criteria become the approved models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the resulting models appear to be satisfactory and to satisfy business needs. It is now appropriate to do a more thorough review of the data mining engagement in order to determine if there is any important factor or task that has somehow been overlooked. This review also covers quality assurance issues—for example: Did we correctly build the model? Did we use only the attributes that we are allowed to use and that are available for future analyses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Review of Process</b>\n",
    "- Summarize the process review and highlight activities that have been missed and those that should be repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are factors that should be repeated:\n",
    "    - XGBClassifier is a very good model that does not require excessive preprocessing and does not have a tendency to overfit on the dataset\n",
    "    - Hyperparameter tuning was helpful in improving model performance\n",
    "    - Mix categorical encoding types between target encoding and onehot encoding\n",
    "    \n",
    "\n",
    "The following are factors that could be improved for future models:\n",
    "    - Feature engineering \n",
    "    - Feature selection in regard to training resources (it doesn't change model performance, but it would improve training times)\n",
    "    - Ensemble models or voting classifiers should be used to balance high-performing models capturing different patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the results of the assessment and the process review, the project team decides how to proceed. The team decides whether to finish this project and move on to deployment, initiate further iterations, or set up new data mining projects. This task includes analyses of remaining resources and budget, which may influence the decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. List of possible actions</b>\n",
    "- List the potential further actions, along with the reasons for and against each option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential further actions are as follows:\n",
    "    - Engineer new features to improve model performance\n",
    "    - Determine which variables can be dropped altogether to improve model training times\n",
    "    - Use an ensemble model or voting classifier to combine the predictive power of multiple high-performing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Decision</b>\n",
    "- Describe the decision as to how to proceed, along with the rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, I would iterate further on the model solution to improve predictive performance, however, I am out of time, so I've gone with the best model trained to this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Base Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Update code base</b>\n",
    "- Are there any updates that should be made to the existing codebase and provided functions based on the code you wrote? (i.e. a new preprocessing function or a new visualization technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the coefficient of variation for every feature in a dataset\n",
    "def coefficient_of_variation(df):\n",
    "    df_cv = df.drop('isfraud',axis=1).std() / df.drop('isfraud',axis=1).mean()\n",
    "    df_cv = df_cv.to_frame().T\n",
    "    df_cv.index = ['Coefficient of Variation']\n",
    "    return df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a histogram for every feature in a dataset\n",
    "def hist_plots(df, bins=100):\n",
    "    #plot_list = []\n",
    "    \n",
    "    for col in df.drop('isfraud', axis=1).columns:\n",
    "        #output_widget = widgets.Output()\n",
    "        \n",
    "        #with output_widget:\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "            # Overall histogram\n",
    "        sns.histplot(df[col], bins=bins, ax=axs[0])\n",
    "        max_count = max([patch.get_height() for patch in axs[0].patches])\n",
    "        axs[0].set_ylim(0, max_count * 1.1)\n",
    "        axs[0].set_title(f\"Histogram of {col}\")\n",
    "\n",
    "            # Histogram when not fraud\n",
    "        sns.histplot(df[df['isfraud'] == 0][col], bins=bins, ax=axs[1])\n",
    "        max_count = max([patch.get_height() for patch in axs[1].patches])\n",
    "        axs[1].set_ylim(0, max_count * 1.1)\n",
    "        axs[1].set_title(f\"Histogram of {col} when not fraud\")\n",
    "\n",
    "            # Histogram when is fraud\n",
    "        sns.histplot(df[df['isfraud'] == 1][col], bins=bins, ax=axs[2])\n",
    "        max_count = max([patch.get_height() for patch in axs[2].patches])\n",
    "        axs[2].set_ylim(0, max_count * 1.1)\n",
    "        axs[2].set_title(f\"Histogram of {col} when is fraud\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots a boxplot for every feature in a dataset\n",
    "def box_plots(df):\n",
    "    #plot_list = []\n",
    "    \n",
    "    for col in df.drop('isfraud', axis=1).columns:\n",
    "        #output_widget = widgets.Output()\n",
    "        \n",
    "        #with output_widget:\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "        # Overall boxplot\n",
    "        sns.boxplot(x=df[col], ax=axs[0])\n",
    "        axs[0].set_title(f\"Boxplot of {col}\")\n",
    "\n",
    "        # Boxplot when not fraud\n",
    "        sns.boxplot(x=df[df['isfraud'] == 0][col], ax=axs[1])\n",
    "        axs[1].set_title(f\"Boxplot of {col} when not fraud\")\n",
    "\n",
    "        # Boxplot when is fraud\n",
    "        sns.boxplot(x=df[df['isfraud'] == 1][col], ax=axs[2])\n",
    "        axs[2].set_title(f\"Boxplot of {col} when is fraud\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns pearson correlations for a full dataframe, the dataframe filtered for the negative class, and the dataframe filtered for the positive class\n",
    "# Plots heatmaps of each correlation\n",
    "def pearson_correlation_heatmaps(df):\n",
    "\n",
    "    df_full_p = df.drop('isfraud', axis=1).corr()\n",
    "    df_not_fraud_p = df[df['isfraud']==0].drop('isfraud', axis=1).corr()\n",
    "    df_fraud_p = df[df['isfraud']==1].drop('isfraud', axis=1).corr()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "    sns.heatmap(df_full_p, annot=False, cmap=\"coolwarm\", center=0, annot_kws={'size': 8}, ax=axes[0])\n",
    "    axes[0].set_title(\"Pearson correlation heatmap of full dataframe\")\n",
    "\n",
    "    sns.heatmap(df_not_fraud_p, annot=False, cmap=\"coolwarm\", center=0, annot_kws={'size': 8}, ax=axes[1])\n",
    "    axes[1].set_title(\"Pearson correlation heatmap of dataframe filtered for not fraud\")\n",
    "\n",
    "    sns.heatmap(df_fraud_p, annot=False, cmap=\"coolwarm\", center=0, annot_kws={'size': 8}, ax=axes[2])\n",
    "    axes[2].set_title(\"Pearson correlation heatmap of dataframe filtered for fraud\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_full_p, df_not_fraud_p, df_fraud_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns spearman correlations for a full dataframe, the dataframe filtered for the negative class, and the dataframe filtered for the positive class\n",
    "# Plots heatmaps of each correlation\n",
    "def spearman_correlation_heatmaps(df):\n",
    "\n",
    "    df_full_s = df.drop('isfraud', axis=1).corr(method='spearman')\n",
    "    df_not_fraud_s = df[df['isfraud']==0].drop('isfraud', axis=1).corr(method='spearman')\n",
    "    df_fraud_s = df[df['isfraud']==1].drop('isfraud', axis=1).corr(method='spearman')\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10,15))\n",
    "\n",
    "    sns.heatmap(df_full_s, annot=False, cmap=\"coolwarm\", center=0, annot_kws={'size': 8}, ax=axes[0])\n",
    "    axes[0].set_title(\"Spearman correlation heatmap of full datagram\")\n",
    "\n",
    "    sns.heatmap(df_not_fraud_s, annot=False, cmap=\"coolwarm\", center=0, annot_kws={'size': 8}, ax=axes[1])\n",
    "    axes[1].set_title(\"Spearman correlation heatmap of dataframe filtered for not fraud\")\n",
    "\n",
    "    sns.heatmap(df_fraud_s, annot=False, cmap=\"coolwarm\", center=0, annot_kws={'size': 8}, ax=axes[2])\n",
    "    axes[2].set_title(\"Spearman correlation heatmap of dataframe filtered for fraud\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return df_full_s, df_not_fraud_s, df_fraud_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Can you suggest further improvements to functions/classes within code base?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score_classification function in data_modeling.py should be updated to accept both predicted class labels and predicted probabilities as arguments, and pass the predicted class labels to the metrics that use class labels and the predicted probabilities to the metrics that use probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
